{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit  # you have everything done for you\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LassoCV, RidgeCV\n",
    " \n",
    " \n",
    "# for time-series cross-validation set 5 folds\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    " \n",
    " \n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    " \n",
    " \n",
    "def timeseries_train_test_split(X, y, test_size):\n",
    "    \"\"\"\n",
    "        Perform train-test split with respect to time series structure\n",
    "    \"\"\"\n",
    " \n",
    "    # get the index after which test set starts\n",
    "    test_index = int(len(X) * (1 - test_size))\n",
    " \n",
    "    X_train = X.iloc[:test_index]\n",
    "    y_train = y.iloc[:test_index]\n",
    "    X_test = X.iloc[test_index:]\n",
    "    y_test = y.iloc[test_index:]\n",
    " \n",
    "    return X_train, X_test, y_train, y_test\n",
    " \n",
    " \n",
    "def plotModelResults(model, X_train, X_test, y_train, y_test, plot_intervals=False, plot_anomalies=False):\n",
    "    \"\"\"\n",
    "        Plots modelled vs fact values, prediction intervals and anomalies\n",
    "    \"\"\"\n",
    "    prediction = model.predict(X_test)\n",
    " \n",
    "    plt.figure(figsize=(15, 7))\n",
    "    plt.plot(prediction, \"g\", label=\"prediction\", linewidth=2.0)\n",
    "    plt.plot(y_test.values, label=\"actual\", linewidth=2.0)\n",
    " \n",
    "    if plot_intervals:\n",
    "        cv = cross_val_score(model, X_train, y_train,\n",
    "                             cv=tscv,\n",
    "                             scoring=\"neg_mean_absolute_error\")\n",
    "        mae = cv.mean() * (-1)\n",
    "        deviation = cv.std()\n",
    " \n",
    "        scale = 20\n",
    "        lower = prediction - (mae + scale * deviation)\n",
    "        upper = prediction + (mae + scale * deviation)\n",
    " \n",
    "        plt.plot(lower, \"r--\", label=\"upper bond / lower bond\", alpha=0.5)\n",
    "        plt.plot(upper, \"r--\", alpha=0.5)\n",
    " \n",
    "        if plot_anomalies:\n",
    "            anomalies = np.array([np.NaN] * len(y_test))\n",
    "            anomalies[y_test < lower] = y_test[y_test < lower]\n",
    "            anomalies[y_test > upper] = y_test[y_test > upper]\n",
    "            plt.plot(anomalies, \"o\", markersize=10, label=\"Anomalies\")\n",
    " \n",
    "    error = mean_absolute_percentage_error(prediction, y_test)\n",
    "    plt.title(\"Mean absolute percentage error {0:.2f}%\".format(error))\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.tight_layout()\n",
    "    plt.grid(True);\n",
    "    plt.savefig(\"linear.png\")\n",
    " \n",
    " \n",
    "def plotCoefficients(model, X_train):\n",
    "    \"\"\"\n",
    "        Plots sorted coefficient values of the model\n",
    "    \"\"\"\n",
    "    coefs = pd.DataFrame(model.coef_, X_train.columns)\n",
    "    coefs.columns = [\"coef\"]\n",
    "    coefs[\"abs\"] = coefs.coef.apply(np.abs)\n",
    "    coefs = coefs.sort_values(by=\"abs\", ascending=False).drop([\"abs\"], axis=1)\n",
    " \n",
    "    plt.figure(figsize=(15, 9))\n",
    "    coefs.coef.plot(kind='bar')\n",
    "    plt.grid(True, axis='y')\n",
    "    plt.hlines(y=0, xmin=0, xmax=len(coefs), linestyles='dashed')\n",
    "    plt.savefig(\"linear-cov.png\")\n",
    " \n",
    " \n",
    "def code_mean(data, cat_feature, real_feature):\n",
    "    \"\"\"\n",
    "    Returns a dictionary where keys are unique categories of the cat_feature,\n",
    "    and values are means over real_feature\n",
    "    \"\"\"\n",
    "    return dict(data.groupby(cat_feature)[real_feature].mean())\n",
    " \n",
    " \n",
    "def prepareData(series, lag_start, lag_end, test_size, target_encoding=False):\n",
    "    \"\"\"\n",
    "        series: pd.DataFrame\n",
    "            dataframe with timeseries\n",
    "        lag_start: int\n",
    "            initial step back in time to slice target variable\n",
    "            example - lag_start = 1 means that the model\n",
    "                      will see yesterday's values to predict today\n",
    "        lag_end: int\n",
    "            final step back in time to slice target variable\n",
    "            example - lag_end = 4 means that the model\n",
    "                      will see up to 4 days back in time to predict today\n",
    "        test_size: float\n",
    "            size of the test dataset after train/test split as percentage of dataset\n",
    "        target_encoding: boolean\n",
    "            if True - add target averages to the dataset\n",
    " \n",
    "    \"\"\"\n",
    "    # copy of the initial dataset\n",
    "    data = pd.DataFrame(series.copy())\n",
    "    data.columns = [\"y\"]\n",
    " \n",
    "    # lags of series\n",
    "    for i in range(lag_start, lag_end):\n",
    "        data[\"lag_{}\".format(i)] = data.y.shift(i)\n",
    " \n",
    "    # datetime features\n",
    "    # data.index = data.index.to_datetime()\n",
    "    data[\"hour\"] = data.index.hour\n",
    "    data[\"weekday\"] = data.index.weekday\n",
    "    data['is_weekend'] = data.weekday.isin([5, 6]) * 1\n",
    " \n",
    "    if target_encoding:\n",
    "        # calculate averages on train set only\n",
    "        test_index = int(len(data.dropna()) * (1 - test_size))\n",
    "        data['weekday_average'] = list(map(\n",
    "            code_mean(data[:test_index], 'weekday', \"y\").get, data.weekday))\n",
    "        data[\"hour_average\"] = list(map(\n",
    "            code_mean(data[:test_index], 'hour', \"y\").get, data.hour))\n",
    " \n",
    "        # frop encoded variables\n",
    "        data.drop([\"hour\", \"weekday\"], axis=1, inplace=True)\n",
    " \n",
    "    # train-test split\n",
    "    y = data.dropna().y\n",
    "    X = data.dropna().drop(['y'], axis=1)\n",
    "    X_train, X_test, y_train, y_test = \\\n",
    "        timeseries_train_test_split(X, y, test_size=test_size)\n",
    " \n",
    "    return X_train, X_test, y_train, y_test\n",
    " \n",
    " \n",
    "def plt_linear():\n",
    "    data = pd.read_csv('raw_data.csv',\n",
    "                                       usecols=['timestamp', 'count'])\n",
    "    data['timestamp'] = pd.to_datetime(data['timestamp'])\n",
    "    data.set_index(\"timestamp\", drop=True, inplace=True)\n",
    "    data.rename(columns={'count': 'y'}, inplace=True)\n",
    " \n",
    "    X_train, X_test, y_train, y_test = \\\n",
    "        prepareData(data, lag_start=6, lag_end=25, test_size=0.3, target_encoding=True)\n",
    " \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    " \n",
    " \n",
    "    # lr = LinearRegression()\n",
    "    lr = LassoCV(cv=tscv)\n",
    "    # lr = RidgeCV(cv=tscv)\n",
    "    # lr.fit(X_train_scaled, y_train)\n",
    " \n",
    "    \"\"\"\n",
    "    from xgboost import XGBRegressor\n",
    "    lr = XGBRegressor()\n",
    "    # lr = xgb.XGBRegressor(max_depth=5, learning_rate=0.1, n_estimators=160, silent=False, objective='reg:gamma')\n",
    "    \"\"\"\n",
    " \n",
    "    lr.fit(X_train_scaled, y_train)\n",
    " \n",
    "    \"\"\"\n",
    "    IMPORTANT\n",
    "    Generally tree-based models poorly handle trends in data, compared to linear models,\n",
    "    so you have to detrend your series first or use some tricks to make the magic happen.\n",
    "    Ideallyâ€Šmake the series stationary and then use XGBoost, for example, you can forecast\n",
    "    trend separately with a linear model and then add predictions from xgboost to get final forecast.\n",
    "    \"\"\"\n",
    " \n",
    "    plotModelResults(lr, X_train=X_train_scaled, X_test=X_test_scaled,  y_train=y_train, y_test=y_test, plot_intervals=True, plot_anomalies=True)\n",
    "    plotCoefficients(lr, X_train=X_train)\n",
    " \n",
    " \n",
    "plt_linear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_linear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
